---
- block:
  - name: Create cluster (check mode)
    oci_container_engine_cluster:
      name: "{{ dependency_cluster_name }}"
      compartment_id: "{{ test_compartment_ocid }}"
      vcn_id: "{{ common_vcn_id }}"
      kubernetes_version: "{{ kubernetes_version }}"
    register: result
    check_mode: yes

  - name: Assert resource would have been created
    assert:
      that:
        - result.changed == True

  - name: Create dependency cluster
    oci_container_engine_cluster:
      name: "{{ dependency_cluster_name }}"
      compartment_id: "{{ test_compartment_ocid }}"
      vcn_id: "{{ common_vcn_id }}"
      kubernetes_version: "{{ kubernetes_version }}"
    register: result

  - name: Assert resource was created
    assert:
      that:
        - result.changed == True

  - set_fact:
      dependency_cluster_id: "{{ result.cluster.id }}"

# ==================================================================================================== #
# This known issue will cause this upgrade to fail: https://gitlab-odx.oracledx.com/oci/ansible-cloud-module/-/issues/827
# This is a bit hacky but we are using this known failure to test retrieving work request errors for container engine
# If this test ever fails, the first step should be to check if the linked issue has been resolved
  - name: Upgrade cluster
    oci_container_engine_cluster:
      cluster_id: "{{ dependency_cluster_id }}"
      kubernetes_version: "{{ result.cluster.available_kubernetes_upgrades[0] }}"
    register: result
    ignore_errors: yes

  - name: Assert that error reason is properly included in response message
    assert:
      that:
        - result is failed
        - "\"failed with errors\" in (result.msg | lower)"
        - "\"failed with errors: None\" not in (result.msg | lower)"

# ==================================================================================================== #

  - name: Create node_pool with subnet_ids and quantity_per_subnet and required parameters
    oci_container_engine_node_pool:
      compartment_id: "{{ test_compartment_ocid }}"
      cluster_id: "{{ dependency_cluster_id }}"
      name: "{{ node_pool_name }}"
      kubernetes_version: "{{ kubernetes_version }}"
      node_image_name: "{{ node_image_name }}"
      node_shape: "{{ node_shape }}"
      subnet_ids: "{{ node_pool_subnet_ids }}"
      quantity_per_subnet: "{{ quantity_per_subnet }}"
    register: result

  - name: Assert that resource was created
    assert:
      that:
        - result.changed == True
        - result.node_pool.compartment_id == test_compartment_ocid
        - result.node_pool.cluster_id == dependency_cluster_id
        - result.node_pool.name == node_pool_name
        - result.node_pool.kubernetes_version == kubernetes_version
        - result.node_pool.node_image_name == node_image_name
        - result.node_pool.node_shape == node_shape
        - result.node_pool.subnet_ids == node_pool_subnet_ids
        - result.node_pool.quantity_per_subnet == quantity_per_subnet
    when: skip_node_pool_subnet_ids_test is not defined

  - set_fact:
      node_pool_id: "{{ result.node_pool.id }}"

# ==================================================================================================== #

  - name: Recreate node_pool with subnet_ids and quantity_per_subnet and required parameters - idempotence
    oci_container_engine_node_pool:
      compartment_id: "{{ test_compartment_ocid }}"
      cluster_id: "{{ dependency_cluster_id }}"
      name: "{{ node_pool_name }}"
      kubernetes_version: "{{ kubernetes_version }}"
      node_image_name: "{{ node_image_name }}"
      node_shape: "{{ node_shape }}"
      subnet_ids: "{{ node_pool_subnet_ids }}"
      quantity_per_subnet: "{{ quantity_per_subnet }}"
    register: result

  - name: Assert that resource was not created
    assert:
      that:
        - result.changed == False
        - result.node_pool.compartment_id == test_compartment_ocid
        - result.node_pool.cluster_id == dependency_cluster_id
        - result.node_pool.name == node_pool_name
        - result.node_pool.kubernetes_version == kubernetes_version
        - result.node_pool.node_image_name == node_image_name
        - result.node_pool.node_shape == node_shape
        - result.node_pool.subnet_ids == node_pool_subnet_ids
        - result.node_pool.quantity_per_subnet == quantity_per_subnet

# ==================================================================================================== #

  - name: Update node_pool name and node_labels
    oci_container_engine_node_pool:
      name: "{{ updated_node_pool_name }}"
      node_pool_id: "{{ node_pool_id }}"
      initial_node_labels: "{{ updated_node_labels }}"
    register: result

  - name: Assert that resource was updated
    assert:
      that:
        - result.changed == True
        - result.node_pool.name == updated_node_pool_name
        - result.node_pool.initial_node_labels == updated_node_labels

# ==================================================================================================== #

  - name: Update node_pool name and node_labels - idempotence
    oci_container_engine_node_pool:
      name: "{{ updated_node_pool_name }}"
      node_pool_id: "{{ node_pool_id }}"
      initial_node_labels: "{{ updated_node_labels }}"
    register: result

  - name: Assert that resource was not updated
    assert:
      that:
        - result.changed == False
        - result.node_pool.name == updated_node_pool_name
        - result.node_pool.initial_node_labels == updated_node_labels

# ==================================================================================================== #

  - name: Delete node_pool
    oci_container_engine_node_pool:
      node_pool_id: "{{ node_pool_id }}"
      state: "absent"
    register: result

  - name: Assert that resource was deleted
    assert:
      that:
        - result.changed == True

# ==================================================================================================== #
# If investigating failures of this test see information here: https://gitlab-odx.oracledx.com/oci/ansible-cloud-module/-/issues/831
# If we see more failures we should file a ticket against the OKE team asking for guidance about what we should do in
# the scenario where the DELETE work request returns successfully but the node_pool still exists and can be fetched through
# the control plane for some time
  - name: Delete node_pool - idempotence
    oci_container_engine_node_pool:
      node_pool_id: "{{ node_pool_id }}"
      state: absent
    register: result

  - name: Assert that resource was not deleted
    assert:
      that:
        - result.changed == False

# ===========================================================================================
# A temporary ssh public key should be generated always as part of this suite to avoid
# any security issues. DB System requires a ssh public key. After finishing the test suite, the
# generated ssh public key is removed.


  - name: create temporary directory for ssh public key
    tempfile:
      state: directory
      suffix: cert
    register: result

  - set_fact:
      ssh_public_key_path: "{{ result.path }}"

  - name: Generate Private Key For using which Public Key will be generated
    openssl_privatekey:
      path: "{{ssh_public_key_path}}/private_key.pem"
      type: RSA
      size: 2048

  - name: Generate Public Key
    openssl_publickey:
      path: "{{ssh_public_key_path}}/public_key.pem"
      privatekey_path: "{{ssh_public_key_path}}/private_key.pem"
      format: OpenSSH
    when: skip_node_pool_node_config_details_test is not defined

  - set_fact:
      ssh_public_key: "{{ lookup('file',  '{{ ssh_public_key_path }}/public_key.pem') }}"

# ==================================================================================================== #

  - name: Create node_pool with node_config_details and required + optional parameters
    oci_container_engine_node_pool:
      compartment_id: "{{ test_compartment_ocid }}"
      cluster_id: "{{ dependency_cluster_id }}"
      name: "{{ node_pool_name }}"
      kubernetes_version: "{{ kubernetes_version }}"
      node_image_name: "{{ node_image_name }}"
      node_shape: "{{ node_shape }}"
      ssh_public_key: "{{ ssh_public_key }}"
      initial_node_labels: "{{ node_labels }}"
      node_config_details: "{{ node_config_details }}"
    register: result

  - name: Assert that resource was created
    assert:
      that:
        - result.changed == True
        - result.node_pool.compartment_id == test_compartment_ocid
        - result.node_pool.cluster_id == dependency_cluster_id
        - result.node_pool.name == node_pool_name
        - result.node_pool.kubernetes_version == kubernetes_version
        - result.node_pool.node_image_name == node_image_name
        - result.node_pool.node_shape == node_shape
        - result.node_pool.ssh_public_key is defined
        - result.node_pool.initial_node_labels == node_labels
        - result.node_pool.node_config_details == node_config_details
    when: skip_node_pool_node_config_details_test is not defined

  - set_fact:
      node_pool_id: "{{ result.node_pool.id }}"

# ==================================================================================================== #

  - name: Recreate node_pool with node_config_details and required + optional parameters - idempotence
    oci_container_engine_node_pool:
      compartment_id: "{{ test_compartment_ocid }}"
      cluster_id: "{{ dependency_cluster_id }}"
      name: "{{ node_pool_name }}"
      kubernetes_version: "{{ kubernetes_version }}"
      node_image_name: "{{ node_image_name }}"
      node_shape: "{{ node_shape }}"
      ssh_public_key: "{{ ssh_public_key }}"
      initial_node_labels: "{{ node_labels }}"
      node_config_details: "{{ node_config_details }}"
    register: result

  - name: Assert that resource was not created
    assert:
      that:
        - result.changed == False
        - result.node_pool.compartment_id == test_compartment_ocid
        - result.node_pool.cluster_id == dependency_cluster_id
        - result.node_pool.name == node_pool_name
        - result.node_pool.kubernetes_version == kubernetes_version
        - result.node_pool.node_image_name == node_image_name
        - result.node_pool.node_shape == node_shape
        - result.node_pool.ssh_public_key is defined
        - result.node_pool.initial_node_labels == node_labels
        - result.node_pool.node_config_details == node_config_details

# ==================================================================================================== #

  - name: Update node_pool name and node_labels
    oci_container_engine_node_pool:
      name: "{{ updated_node_pool_name }}"
      node_pool_id: "{{ node_pool_id }}"
      initial_node_labels: "{{ updated_node_labels }}"
    register: result
    when: skip_node_pool_node_config_details_test is not defined

  - name: Assert that resource was updated
    assert:
      that:
        - result.changed == True
        - result.node_pool.name == updated_node_pool_name
        - result.node_pool.initial_node_labels == updated_node_labels

# ==================================================================================================== #

  - name: Update node_pool name and node_labels - idempotence
    oci_container_engine_node_pool:
      name: "{{ updated_node_pool_name }}"
      node_pool_id: "{{ node_pool_id }}"
      initial_node_labels: "{{ updated_node_labels }}"
    register: result

  - name: Assert that resource was not updated
    assert:
      that:
        - result.changed == False
        - result.node_pool.name == updated_node_pool_name
        - result.node_pool.initial_node_labels == updated_node_labels

# ==================================================================================================== #

  - name: Fetch all node_pools
    oci_container_engine_node_pool_facts:
      compartment_id: "{{ test_compartment_ocid }}"
    register: result

  - name: Assert resource was listed
    assert:
      that:
        - result.node_pools | length > 0

# ==================================================================================================== #

  - name: Get node_pool
    oci_container_engine_node_pool_facts:
      node_pool_id: "{{ node_pool_id }}"
    register: result

  - name: Assert resource fetched
    assert:
      that:
        - result.node_pools | length == 1

# ==================================================================================================== #

  - name: Delete node_pool
    oci_container_engine_node_pool:
      node_pool_id: "{{ node_pool_id }}"
      state: "absent"
    register: result

  - name: Assert that resource was deleted
    assert:
      that:
        - result.changed == True

# ==================================================================================================== #
# If investigating failures of this test see information here: https://gitlab-odx.oracledx.com/oci/ansible-cloud-module/-/issues/831
# If we see more failures we should file a ticket against the OKE team asking for guidance about what we should do in
# the scenario where the DELETE work request returns successfully but the node_pool still exists and can be fetched through
# the control plane for some time
  - name: Delete node_pool - idempotence
    oci_container_engine_node_pool:
      node_pool_id: "{{ node_pool_id }}"
      state: absent
    register: result

  - name: Assert that resource was not deleted
    assert:
      that:
        - result.changed == False

  - name: Delete dependency cluster
    oci_container_engine_cluster:
      cluster_id: "{{ dependency_cluster_id }}"
      state: absent
    register: result

  - name: Assert that the cluster is deleted
    assert:
      that:
        - result.changed == True
# ==================================================================================================== #

  rescue:
    - name: Clean artifact path
      file:
        state: absent
        path: "{{ssh_public_key_path}}"
      ignore_errors: yes

    - name: Delete node_pool
      oci_container_engine_node_pool:
        node_pool_id: "{{ node_pool_id }}"
        state: absent
      ignore_errors: yes

    - name: Delete dependency cluster
      oci_container_engine_cluster:
        cluster_id: "{{ dependency_cluster_id }}"
        state: absent
      ignore_errors: yes

    - fail:
        msg: "{{ ansible_failed_result }}"
